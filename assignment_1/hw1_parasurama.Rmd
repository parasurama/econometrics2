---
title: "Problem Set 1"
author: "Prasanna Parasurama"
date: "Due: 2/15/19"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(results = "hold")
```

```{r}
library(tidyverse)
library(dplyr)
library(lubridate)
library(ggplot2)
library(grid)
library(zoo)
library(dynlm)
```

## Problem 1 (Analytical Exercise)

Consider a simple $AR(1)$ model:
\begin{eqnarray*}
Y_{t} = \alpha Y_{t-1} + \varepsilon_{t} \qquad \mbox{ with } \varepsilon_t \sim N(0,\sigma^2) \mbox  { for } t=\{1,\ldots,T\} \mbox{ and } Y_0 = 0
\end{eqnarray*}

\begin{enumerate}
\item What is the distribution of $Y_1$? What is the distribution of  $Y_2$?

Distribution of $Y_1$

$$Y_1 = \alpha Y_0 + \epsilon_1$$
Since $Y_0$ = 0,
$$Y_1 = \epsilon_1 \sim N(0, \sigma^2)$$

Distribution of $Y_2$
$$Y_2 = \alpha Y_1 + \epsilon_2$$
$$Y_1 \sim N(0, \sigma^2)$$
$$\alpha Y_1 \sim N(0, \alpha^2\sigma^2)$$
Since $Y_2$ is a sum of 2 random normal variables,

$$Y_2 \sim N(0, \alpha^2\sigma^2 + \sigma^2) = N(0, \sigma^2 (1+ \alpha^2))$$

\item What is the distribution of $Y_{t}$ for $|\alpha |< 1$ as $t\to\infty$.

We can generalize the distribution for $Y_t$ as follows:
$$Y_t \sim N(0, \sigma^2 (1+ \alpha^2 + \alpha^4 + ...\alpha^{2t}))$$

The power series converges to $\frac{1}{1-\alpha^2}$. Therefore:
$$Y_t \sim N(0, \sigma^2\frac{1}{1-\alpha^2}), t\to\infty $$

\item What is the definition of stationarity? Explain why in this model we can check for stationarity by looking at the mean and the variance of the $Y_t$.

Stationarity means that the distribution of $Y$ is invariant to time. In this model, since $Y$ is normally distributed, it is fully characterized by its mean and variance. If the mean and variance are independent of time, then the series is stationary. 

\item Suppose that $\alpha = 1$.  Why does this imply that the model is nonstationary? Can you think of a simple transformation that makes the model stationary?

Yes, $\alpha = 1$ means that the variance increases with time. 

Taking the first difference $\Delta Y = Y_t - Y_{t-1} =  \epsilon_t$ will make this stationary. 

\item Now suppose that  $|\alpha |< 1$. Find a formula for the $j$th autocorrelation $\rho_j = corr(Y_t,Y_{t-j})$.

$$\rho_j = corr(Y_t,Y_{t-j}) = \frac{Cov(Y_t,Y_{t-j})}{Var(Y_t)}$$
Consider $Y_t$ when it is stationary (as t becomes large). From part (2), we know that:

$$Var(Y_t) = \frac{\sigma^2}{1-\alpha^2}$$
$$Cov(Y_t,Y_{t-j}) = E[Y_t Y_{t-j}]-E[Y_t]E[Y_{t-j}] = E[Y_tY_{t-j}]$$

Consider a simple case, where $j=1$

$$Cov(Y_t,Y_{t-1}) = E[Y_tY_{t-1}]$$
Substituting the AR(1) model formula for $T_t$:
$$E[Y_tY_{t-1}] = E[(\alpha Y_{t-1} + \epsilon_t)Y_{t-1}] = E[\alpha Y_{t-1}Y_{t-1}] + E[\epsilon_tY_{t-1}]$$


Note that $E[\epsilon_tY_{t-1}] = 0$, and $E[\alpha Y_{t-1}Y_{t-1}]$ is simply $\alpha Var(Y_{t-1})$

Since Y is stationary,
$$Var(Y_{t-1}) = Var(Y_{t})$$
$$Cov(Y_t,Y_{t-1}) = \alpha Var(Y_t) = \frac{\alpha\sigma^2}{1-\alpha^2}$$

If we recurse for any j as in part(1), we can get the general formula:
$$Cov(Y_t,Y_{t-j}) = \frac{\alpha^j\sigma^2}{1-\alpha^2}$$

Plugging these back into the original equation:
$$\rho_j = \frac{Cov(Y_t,Y_{t-j})}{Var(Y_t)} = \frac{\alpha^j\sigma^2}{1-\alpha^2} \frac{1}{\frac{\sigma^2}{1-\alpha^2}} = \alpha^j$$

\item Explain how we could use estimates of $\rho_j$ for $j=1,2,\ldots$ to check whether some actual time series data was generated by an AR(1) model like we one described above.

Estimate the sample correlation coeffecient:
$$\hat{\rho}(j) = \frac{1}{T}\sum\limits_{t=j+1}^{T}{(Y_t - \bar{Y})(Y_{t-j} - \bar{Y})}$$

$\hat{\rho}(j)$ will tend to 0, as $j$ increases. That is, current data will be less and less predictive the further we go into the future. 

\end{enumerate}

## Problem 2 (Coding Exercise)

The problem will take you through a few tasks to familiarize yourself with R, as well as, some basic time series concepts:

(a) Loading data into R
(b) Doing simple data analysis 
(c) Doimg time series analysis 

For this problem, we have pulled two seperate datasets from the FRED database, maintained by the Federal Reserve Bank of Saint Louis (https://fred.stlouisfed.org/). The datasets cover the aggregate revenue and load factor in domestic US flights from 2000 to 2018. In the last two decades, airlines have begun using sophisticated algorithms to increase capacity utilization of flights (i.e. flights tend to be more full). Furthermore, during the same time period, airline revenues have increased. The point of this exercise will be to understand the role of these productivity increases in "explaining" increased revenues in the airline industry. 

The two seperate datasets you will be working with are:

1. US Domestic Air Travel Revenue Passenger Mile (**filename = us_air_rev.csv**) : this dataset contains monthly data detailing the number of miles traveled by paying passengers in domestic US air travel.
2. US Domestic Air Travel Load Factor (**filename = us_load_factor.csv**) : this dataset contains monthly data detailing the percentage of seats filled up (capacity utilitization) in domestic US air travel.


## (a) Loading Data

The first thing we want you to do is to load both datasets: **us_air_rev.csv** and **us_load_factor.csv** into R. 

**Please load data in the section below**


```{r,results='hold',comment=''}
## ~~ Problem 2: Part (a) Load Data into R ~~ ##

## Load Air Revenue Data ##
df_rev = read_csv("../data/us_air_rev.csv")
## Load Air Load Factor Data ##
df_load = read_csv("../data/us_load_factor.csv")

#convert to date
df_rev = df_rev %>% mutate(date=as.Date(date, format="%m/%d/%Y"))
df_load = df_load %>% mutate(date=as.Date(date, format="%m/%d/%Y"))
```

There are two ways to view data that you have loaded into memory in R.

1. View only first (or last few rows) using `head` (`tails`) commands
2. View the entire dataset in a seperate window using `View` commands

Note, for very large datasets it is not a good idea to use the `View` command as it is very memory (RAM) intensive. 

Other checks you always want to do when loading data includes:

1. Check the column names using `colnames`
2. Check the data types for each column using a loop and `xxx` 
3. Check the dimension (number of rows and columns) using the `dim` command

**We now want you to run the following checks on both of your loaded datasets**:

(1) Print the column names.
```{r, results="hold"}
print(colnames(df_rev))
print(colnames(df_load))
```

(2) Print off the first 20 rows.
```{r, results="hold"}
head(df_rev, 20)
head(df_load, 20)
```


(3) Print off the number of rows and columns.
```{r, results="hold"}
dim(df_rev)
dim(df_load)
```


(4) Print the data types of all the columns.

Note, for part (4) I have already built the for loop statement to get all the data types for each of the columns. For those familiar with `for loops` in other environments, R has a built in set of `apply` functions that are optimized for specific objects (`lapply` is optimized for lists, `vapply` is optimized for vectors etc). If you are unfamiliar with `for loops`, give it a google. 

```{r}
lapply(df_rev, typeof)
lapply(df_load, typeof)
```


## (b) Doing simple data analysis

In the next part, we will have you doing some actual time series analysis. But generally we are interested in decomposing time series into trend, seasonal and stochastic components. One clear form of seasonality is month to month variation in the data. An "approximation" for trend components is to look at year to year changes. We will have you investigate these below. 

**We now want you to do the following**:

(1) Calculate the average revenue and load factor, by year. Do this two ways: (1) Using `aggregate` and `mean`, (2) Using `aggregate` and `sum`. 

```{r,results='hold',comment=''}
print("Avg Yearly Revenue")
avg_yearly_rev = df_rev %>% group_by("year"=year(date)) %>% summarise("avg_revenue"=mean(total_rev))
avg_yearly_rev
df_rev %>% group_by("year"=year(date)) %>%
        summarise("avg_revenue"=sum(total_rev)/length(total_rev))

print("Avg Yearly Load")
avg_yearly_load = df_load %>% group_by("year"=year(date)) %>% summarise("avg_load"=mean(load_factor))
avg_yearly_load
print(df_load %>% group_by("year"=year(date)) %>% summarise("avg_load"=sum(load_factor)/length(load_factor)))

```

(2) Calculate the average revenue and load factor, by month. Do this two ways: (1) Using `aggregate` and `mean`, (2) Using `aggregate` and `sum`. 

```{r}
print("Avg Monthly Revenue")
avg_monthly_rev = df_rev %>% group_by("month"=month(date)) %>%
  summarise("avg_revenue"=mean(total_rev))
avg_monthly_rev
df_rev %>% group_by("month"=month(date)) %>%
        summarise("avg_revenue"=sum(total_rev)/length(total_rev))


print("Avg Monthly Load")
avg_monthly_load = df_load %>% group_by("month"=month(date)) %>%summarise("avg_load"=mean(load_factor))
avg_monthly_load
df_load %>% group_by("month"=month(date)) %>% summarise("avg_load"=sum(load_factor)/length(load_factor))

```

(3) Plot graphs for part (1) and (2) on the same plot, using your favorite plotting function. Note, you can either use the built-in `plot` function or the popular external library `ggplot2`. 

For parts (1) and (2), I want you to build a better understanding of using R. I am asking you to compute averages using two different methods. In Method (1), you can use the built-in `mean` function to have R do the work for you. In Method (2), you will do the average calculation yourself by summing over observations and dividing by the number of observations. 
```{r}
p1 = ggplot() + geom_line(data=avg_yearly_rev, aes(x=year, y=avg_revenue)) + theme(axis.text.x = element_blank(), axis.title.x = element_blank())
p2 = ggplot() + geom_line(data=avg_yearly_load, aes(x=year, y=avg_load)) 
grid.newpage()
grid.draw(rbind(ggplotGrob(p1), ggplotGrob(p2), size="last"))


p3 = ggplot() + geom_line(data=avg_monthly_rev, aes(x=month, y=avg_revenue)) + theme(axis.text.x = element_blank(), axis.title.x = element_blank())
p4 = ggplot() + geom_line(data=avg_monthly_load, aes(x=month, y=avg_load)) 
grid.newpage()
grid.draw(rbind(ggplotGrob(p3), ggplotGrob(p4), size="last"))

```

## (c) Doing time series analysis

In R, there are already built-in functions that allow us to do these seasonality and trend decompositions with much fewer lines of code. To do so, we must convert our data into time series objects. What seperates a normal vector of data from a vector of time series data is that the latter has some time frequency of observations. In our case, the time frequency is monthly. 

We want now return to the main question of this section, how much does capacity utilizations explain increases in airline revenue? 

Fixing notation, we have:

* $t \in \{01/2000,\ldots,12/2018\} = T$ is month-year combinations
* $Rev_{t}$ is revenue for each month-year combination
* $Load_{t}$ is load factor for each month-year combination

**We now want you to do the following**:

(1) Create a time series object using the `ts` command for each series. Be sure to specify the correct frequency for the data.
```{r}
# convert to timeseries
ts_rev = ts(df_rev$total_rev, start = c(2000,1), end = c(2018,7), frequency = 12)
ts_load = ts(df_load$load_factor, start = c(2000,1), end = c(2018,9), frequency = 12)

```

(2) Plot an autocorrelation function between our two time series: $\{Rev_{t}\}_{t\in T}$ and $\{Load_{t}\}_{t\in T}$.
```{r}
# autocorrelation
acf(ts_rev, lag.max = 300)
acf(ts_load, lag.max = 300)

```


(3) Run the following linear regression, reporting the coefficients and $R^{2}$:
\begin{align*}
  Rev_{t} = \alpha + \beta Load_{t} + \epsilon_{t}
\end{align*}

```{r}
# regression
df = inner_join(df_rev, df_load, by=c("date"))
m.lm = lm(total_rev ~ load_factor, data=df)
print(summary(m.lm))

```

(4) Decompose both series into cyclical and trend components using the `decompose` command. Plot seperately these cyclical and trend components for each of the series.

```{r,results='hold',comment=''}
# decompose
dc_load = decompose(ts_load)
dc_rev = decompose(ts_rev)

plot(dc_load$seasonal)
plot(dc_load$trend)

plot(dc_rev$seasonal)
plot(dc_rev$trend)

```

(5) Using the dataframe created in part (3), redo parts (2) and (3). What differs from part (2)? Why? What can we conclude about the impact of capacity utilization changes on revenues?

```{r}
acf(dc_rev$random, lag.max = 300, na.action = na.pass)
acf(dc_load$random, lag.max=300, na.action = na.pass)

# join dataframes
rev_random = data.frame(revenue=as.matrix(dc_rev$random), date=time(dc_rev$random))
load_random = data.frame(load=as.matrix(dc_load$random), date=time(dc_load$random))
df_random = inner_join(rev_random, load_random, by=c("date"))

m.random_lm = lm(revenue ~ load, data=df_random)
summary(m.random_lm)
```

Seasonal and trend effects are mostly gone - i.e most of the bars are within dotted blue lines. In load factors, there's some artifacts of seasonality. 

Even after taking out trend and seasonality, load factor is still predictive of revenue miles. Although, the estimate is much lower. A 0.1 increase in load factor corresponds to 42348 increase in revenue miles.

Below is a scatter plot of disturbences of revenue vs. load.

```{r}
df_random %>% ggplot() + geom_point(aes(x=load, y = revenue))
```

## Problem 3 (Coding Exercise)

In class, you have learned about the Wold decomposition, a fundamental result in time series analysis. This exercise will attempt to walk through Wold's theorem in practice. We have provided simulated time series data, in an .rda file called "ts_simulation.rda", where $Y_{t}$ is the t$^{th}$ observation from our data. To open this file, use the load command. The name of the dataframe is "sim". 

```{r}
load("../data/ts_simulation.rda")

```

**We now want you to do the following**:

(1) Verify the stationarity of the process. Do this in two ways:

(a) "Heuristic" : show that the first-moment and second-moment do not depend on $t$.

We can select a window size (say, H=3). The mean and the variance should be randomly dispersed, as this window moves over time: 
```{r}
plot(zoo::rollapply(sim$Y_t, 3, mean))
plot(zoo::rollapply(sim$Y_t, 3, var))
```
We see that the both mean and variance are indeed randomly dispersed. 

(b) "Testing" : use a Dickey-Fuller test to test for stationarity. Interpret your results.

We can test the following model, with the null that $\delta = 0$ with a one-sided test

$$\Delta Y_t = \beta_0+\delta Y_{t-1} + \epsilon_t $$

```{r}
sim_ts = ts(sim$Y_t)
summary(dynlm(d(sim_ts) ~ L(sim_ts,1)))
```

Since $\delta$ is negative and not 0, the series is stationary. 
(Question for later: We do a one-sided test. Why can delta not be positive?)

(2) Estimate three seperate autoregressive models: AR(1), AR(3) and AR(6). For each of the seperate models, retrieve the residuals, $\hat{\epsilon}_{\{t,p\}}$, where $p$ is the order of the AR process. Using each set of residuals of the AR process, estimate an MA(2) model, where $\hat{\eta}_{\{t,p,q\}}$ are the residuals of this second step.

|     Verify whether the assumptions of Wold are violated:

\begin{align*}
  Corr[\hat{\epsilon}_{\{t,p\}}Y_{s}] &= 0 \mbox{ such that } s < t \\
  E[\eta_{\{t,p,q\}}] = 0 \\
  Var(\eta_{\{t,p,q\}}) = \sigma^{2}
\end{align*}

Estimate AR(1), AR(3) and AR(6)
```{r}
ar1 = ar(sim_ts, order.max = 1)
ar3 = ar(sim_ts, order.max = 3)
ar6 = ar(sim_ts, order.max = 6, aic=FALSE)
```

MA(2) can be estimated with residuals from AR(1) and AR(2):
```{r}
ar2 = ar(sim_ts, order.max = 2)

ma2 = dynlm(sim_ts ~ ar1$resid + ar2$resid)
summary(ma2)
```

We can now test the Wold assumptions below. 

Correlation assumtion:
```{r}
# correlation assumtion
cor(ar1$resid, data.table::shift(sim_ts, 1), use="complete.obs")
cor(ar3$resid, data.table::shift(sim_ts, 3), use="complete.obs")
cor(ar6$resid, data.table::shift(sim_ts, 6), use="complete.obs")
```

Note that they are all close to 0. 

Expectation assumtion:
```{r}
mean(ma2$residuals)
```

Expectation of residuals is 0. 

```{r}
var(ma2$residuals)
```

Not sure what $\sigma^2$ is supposed to be...

(3) To find the right ARMA($p$,$q$) process, we add new lags (increase $p$), estimate our model, use an information criteria to determine the increase in fit and stop once new models do not improve fit. To simplify the problem, assume $q=2$. Build a series of ARMA($p$,$q$) models, using the Akaike Information Criteria (AIC) to find the right $p$. (Note: A `for loop` over $p$ would be a good idea).

```{r,results='hold',comment=''}
for (p in c(1:6)) {
  arma = arima(sim_ts, order=c(p, 0, 2))
  print(paste("p=", p, " ", "AIC=", arma$aic))
}
```

$p=1$ gives a low AIC. 