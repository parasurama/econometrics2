\documentclass[xcolor=pdftex,dvipsnames,table,mathserif]{beamer}
\usetheme{default}
%\usetheme{Darmstadt}
%\usepackage{times}
%\usefonttheme{structurebold}

\usepackage[english]{babel}
%\usepackage[table]{xcolor}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{amsmath,amssymb,setspace,centernot,MnSymbol}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{relsize}
\usepackage{pdfpages}
\usepackage[absolute,overlay]{textpos} 


\newenvironment{reference}[2]{% 
  \begin{textblock*}{\textwidth}(#1,#2) 
      \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}} 

\DeclareMathSizes{10}{10}{6}{6} 

\begin{document}
\title{Part 12: Trees, Boosting, and Bagging}
\author{Chris Conlon}
\institute{Applied Econometrics II}
\date{\today}

\frame{\titlepage}


\begin{frame}
\frametitle{Reading}
\begin{itemize}
\item Chapters X of \textit{Elements of Statistical Learning}
\end{itemize}
\end{frame}



\section{Trees}
\begin{frame}
\frametitle{Decision Trees}
Start with $y = f(x_i)$:
\begin{itemize}
\item Construct a \alert{tree} by \alert{splitting} the sample on an $x_i$.
\begin{itemize}
\item Choose the split to maximize the criterion function.
\item Choose the $x_i$ to maximize the criterion given the proposed split.
\end{itemize}
\item Which $x_i$ do we choose?
\begin{itemize}
\item There are $K$ possibilities. 
\item But how do we know which order to split?
\end{itemize}
\item Which split do we choose?
\begin{itemize}
\item This is usually single dimensional optimization.
\end{itemize}
\item Resulting problem is $NP$ hard.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Decision Trees}
What kind of tree?
\begin{itemize}
\item \alert{Classification} Trees predict \alert{discrete} outcomes.
\item \alert{Regression} Trees predict \alert{continuous} outcomes.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Decision Trees: Titanic Survival}
\includegraphics[width=2.5in]{./resources/titanic.png}
\end{frame}


\begin{frame}
\frametitle{Decision Trees}
\tiny
What is the criteria function?
\begin{itemize}
\item RSS: $\min_f \sum_{i=1}^N (y - f(x_i))^2$
\item Maximize Variance Reduction
\begin{align*}
I_{V}(N)=\underbrace{\frac{1}{|S|^{2}} \sum_{i \in S} \frac{1}{2}\left(x_{i}-x_{j}\right)^{2}}_{\text{Pre split Var}}\\
-\left( \underbrace{\frac{1}{\left|S_{t}\right|^{2}} \sum_{i \in S_{t}} \sum_{j \in S_{t}} \frac{1}{2}\left(x_{i}-x_{j}\right)^{2}}_{\text{Post split  TRUE}}
+\underbrace{\frac{1}{\left|S_{f}\right|^{2}} \sum_{i \in S_{f}} \sum_{j \in S_{f}} \frac{1}{2}\left(x_{i}-x_{j}\right)^{2}}_{\text{Post split FALSE}}
\right)
\end{align*}
\item Gini Impurity: Randomly label an element, odds of being correct? $\sum_j p_j \times ( \sum_{k \neq j} p_k ) =\sum_j p_j (1 -p_j) = 1 -\sum_j p_j^2  $ 
\item Information Gain/Entropy: $$\mathrm{H}(T)=\mathrm{I}_{E}\left(p_{1}, p_{2}, \ldots, p_{J}\right)=-\sum_{i=1}^{J} p_{i} \log _{2} p_{i}$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{What are trees actually doing?}
Think about the $RSS$ case : $\min_f \sum_{i=1}^N (y - f(x_i))^2$
\begin{itemize}
\item This is \alert{nonparametric regression}
\item It is also a \alert{kernel model}.
\item Partition the data: $\left\{R_{1}, \ldots, R_{M}\right\}$ into \alert{leaves} that are \alert{disjoint} and \alert{span the space} of $X$.
\item $ f(x)=\sum_{m=1}^{M} c_{m} 1\left(x \in R_{m}\right)$. 
\begin{itemize}
\item Where $c_m$ is mean of $y_i$ within leaf $R_m$.
\end{itemize}
\item This is just a \alert{locally constant} regression. But distance is not determined using bandwidth...
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{What are trees actually doing?}
When are trees the right tool?
\begin{itemize}
\item Because of multiple levels of splits: work best when true relationship is highly nonlinear.
\item Who has a heart attack?
\begin{itemize}
\item Lots of factors: high BP, overweight, age, family history, diabetes, etc.
\item  Logit:  these enter \alert{additively} in the index and increase \alert{log odds proportionally}.
\item Can interact multiple factors $highBP \times overweight$, etc.
\end{itemize}
\item If the true model is highly interacted: trees will do well.
\item In general trees have \alert{low bias} but \alert{high variance}
\begin{itemize}
\item Small changes in data can lead to wildly different splits (and trees).
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Linear vs. Nonlinear Relationships}
\includegraphics[width=3.5in]{./resources/treefit.png}
\end{frame}


\begin{frame}
\frametitle{How do splits work?}
\begin{itemize}
\item Binary variables: Trivial
\item Continuous Variables: Choose a split value $s$ so that $x>s$ or $x\leq s$ optimizes your criteria.
\begin{itemize}
\item This is a single dimensional search (Golden Section, etc.).
\end{itemize}
\item Multiple Categories $(A,B,C)$: this is the hard case
\begin{itemize}
\item Order by the outcome variable $y_b > y_a > y_c$.
\item Then treat like the continuous case.
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Growing your Tree}
\begin{itemize}
\item Let $|T| = M$ denote the number of terminal nodes in $T$ . We will use $|T|$ to measure the complexity of a tree. For any given complexity,
we want the tree minimizing square error on training set.
\item Finding the optimal binary tree of a given complexity is computationally intractable (NP hard).
\end{itemize}
\end{frame}


\begin{frame}{Tree Algorithms}
How do we build a tree?
\begin{itemize}
\item Because the problem is $NP$ hard: no ideal way.
\item Mostly heuristics.
\item Greedy Algorithm: Choose best split first for best $x$ go from there.
\begin{itemize}
\item Need not be optimal...
\end{itemize}
\item Avoiding \alert{over-fitting}
\begin{itemize}
\item How deep to make tree?
\item Minimum number of observations per branch?
\item Pruning?
\item Trees will always overfit if you let them!
\end{itemize}
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Overfitting: Bias Variance}
\includegraphics[width=2.5in]{./resources/bias-variance.png}
\end{frame}


\begin{frame}{Why Prune a Tree?}
To avoid overfitting
\begin{itemize}
\item Any useful split will be made (eventually).
\item We (may) end up with perfectly predicted outcomes $E[y_i | x_i]$.
 case we know for sure that $y_i=1$ ?
\item Remove leaves with too few elements.
\item Usually use a hold-out sample (test set) and remove leaves if it increases OOS criteria.
\item Other canned pruning algorithms will generate several candidate \alert{subtrees}
\begin{itemize}
\item Used out of sample critieria to pick the best subtree
\end{itemize}
\item Can also do \alert{early stopping}
\end{itemize}
\end{frame}



\begin{frame}{Tree Algorithms}
About \alert{criteria} and \alert{heuristics}.
\begin{itemize}
\item ID3 (Iterative Dichotomiser 3)
\item C4.5 (successor of ID3)
\item CART (Classification And Regression Tree)
\item Chi-square automatic interaction detection (CHAID). Performs multi-level splits when computing classification trees.
\item MARS: extends decision trees to handle numerical data better.
\item Conditional Inference Trees. Statistics-based approach that uses non-parametric tests as splitting criteria, corrected for multiple testing to avoid overfitting. This approach results in unbiased predictor selection and does not require pruning.
\end{itemize}
\end{frame}



\section{Bagging}
\begin{frame}{Bagging}
What is \alert{Bagging}? \alert{B}ootstrap \alert{Agg}regation.
\begin{itemize}
\item We re-sample a new dataset the size of our original dataset \alert{with replacement}
\item Before we re-ran our estimation to get $\hat{\theta}^b$ and took $Var(\hat{\theta}^{(1)},\hat{\theta}^{(2)},\ldots,\hat{\theta}^{(B)})$ and $E(\theta)=\frac{1}{B} \sum_{b=1}^B \hat{\theta}^b$.
\item Since we are doing ML, let's bag $\hat{y}_i$.
\item $E(y )=\frac{1}{B} \sum_{b=1}^B \hat{y}^b$ (we could also do this conditionally).
\end{itemize}
\end{frame}

\begin{frame}{Bagging: What's the point?}
Why would we want to do this?
\begin{itemize}
\item If we are worried about high-variance and \alert{overfitting} bagging will reduce our variance.
\item Each bootstrapped sample $b$ is like an IID realization of our data.
\item Averaging over  samples can reduce the variance of $\hat{y}$ by $\frac{1}{B}$.
\end{itemize}
The problem \alert{Interpretability}
\begin{itemize}
\item If my ``model'' is OLS I can report average coefficients.
\item If my model is a tree, what do I even report averaged over 1000 bootstrapped simulations?
\end{itemize}
\end{frame}

\begin{frame}{How to interpret variables?}
\begin{itemize}
\item Pick a tree $T^b$ calculate how much a particular variable $x_1$ increases the Gini Index, or decreases RSS, etc.
\item Average over all trees (some trees will be zero if they don't include $x_1$).
\item Report the average in \alert{relative} terms for all $x_k$.
\end{itemize}

\end{frame}


\begin{frame}
\frametitle{Variable Importance Plots}
\includegraphics[width=4in]{./resources/Importance.jpg}
\end{frame}

\begin{frame}
\frametitle{Variable Importance Plots}
\includegraphics[width=4in]{./resources/variable-importance.png}
\end{frame}


\begin{frame}{Validation: How do assess model fit?}
Out of Bag Error Rate (OOB)
\begin{itemize}
\item Use \alert{cross validation}
\item Draw a bootstrap sample of size $N$.
\item Split sample into two parts: training $\frac{2}{3}$, test $\frac{1}{3}$
\item Fit on the training sample
\item Predict on the test sample: $(y_i - \hat{y}_i^{OOB})^2$
\item Report average or (mean-squared) error on the test sample only.
\item Gives us expected out of sample fit.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Bagging: OOB error}
\includegraphics[width=4in]{./resources/bagging.png}
\end{frame}

\begin{frame}
\frametitle{Why do Bagging do so poorly?}
\begin{itemize}
\item Even though we bootstrap our sample many times $\rightarrow$ $(T^B,T^{B'})$ are highly correlated.
\item Why does this happen?
\begin{itemize}
\item We end up with the same $X$'s and the same splits in each model.
\end{itemize}
\item Adding more trees doesn't really improve forecast performance.
\item We would like to add more trees but have them be \alert{less correlated} with one another.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Solution: Random Forests}
\begin{enumerate}
\item Draw a bootstrap sample $b$.
\item Fit a tree (usually a small one with limited depth)
\item At each node select a \alert{random subset} of regressors $m < K$ from $X$.
\item Average the predictions of (hopefully) less correlated trees. (Or Majority Vote).
\end{enumerate}
How to choose $m$? Usually $\sqrt{K}$.
\end{frame}



\begin{frame}
\frametitle{Solution: Random Forests}
\begin{itemize}
\item Even though we bootstrap our sample many times $\rightarrow$ $(T^B,T^{B'})$ are highly correlated.
\item Adding more trees doesn't really improve forecast performance.
\item We would like to add more trees but have them be \alert{less correlated} with one another.
\end{itemize}
\end{frame}

\begin{frame}{Evaluating Random Forests}
How important is $X^{(j)}$? Look at the trees that randomly don't include it in $m$ (or don't select it)
\begin{align*}
\widehat{\Delta}_{j}=\frac{1}{m} \sum_{i \in \mathcal{H}} \left(Y_{i}-\widehat{m}_{(-j)}\left(X_{i}\right)\right)^{2}-\left(Y_{i}-\widehat{m}\left(X_{i}\right)\right)^{2}
\end{align*}
Average this quantity across the entire training set:
\begin{align*}
\mathbb{E}\left[\widehat{\Delta}_{j} | \mathcal{T}\right]=\mathbb{E}\left[\left(Y-\widehat{m}_{(-j)}(X)\right)^{2}-(Y-\widehat{m}(X))^{2} | \mathcal{T}\right] \equiv \Delta_{j}
\end{align*}
This is called \alert{Leave-One-Out-COvariates} (LOCO).
\end{frame}


\begin{frame}{Inference on Random Forests}
\begin{itemize}
\item Technically possible but tricky
\item Active area of research
\item See Wager and Athey (2017).
\end{itemize}
\end{frame}


\begin{frame}{Inference on Random Forests}
\begin{itemize}
\item Technically possible but tricky
\item Active area of research
\item See Wager and Athey (2017).
\end{itemize}
\end{frame}


\begin{frame}{What about Boosting?}
\alert{Boosting} works somewhat differently
\begin{enumerate}
\item Set $\hat{f}(x) = 0$ and $r_i = y_i$ for $i=1,\ldots, N$.
\item For $b=1,\ldots,B$ iterate on:
\begin{enumerate}
\item Fit a tree $\hat{f}^b$ with $d$ splits to the response $r_1,\ldots, r_n$.
\item Update the prediction to
\begin{align*}
\hat{f}(x) \leftarrow \hat{f}(x)+\lambda \hat{f}^{b}(x)
\end{align*}
\item Update the residual
\begin{align*}
r_{i} \leftarrow r_{i}-\lambda \hat{f}^{b}\left(x_{i}\right)
\end{align*}
\end{enumerate}
\item Produce the final model by averaging
\begin{align*}
\hat{f}(x)=\sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
\end{align*}
\end{enumerate}
\end{frame}


\begin{frame}{How does Boosting Work?}
\begin{itemize}
\item At each bootstrap iteration we aren't fitting $y_i$
\item We are fitting $r_i$ the residual $y_i - \sum_{b'=1}^{b} \lambda \hat{f}^{b'}$. 
\item $\lambda$ is usually small so that we update the model \alert{slowly}.
\item You can think about what we are doing as \alert{re-weighting our training data}
\begin{itemize}
\item Put more weight on the cases we fit the worst (large residuals).
\end{itemize}
\item Usually set $\lambda$ small $.01$
\item Well known algorithm \alert{AdaBoost}
\end{itemize}
Boosting $>$ Bagging $>$ Single Tree
\end{frame}

\begin{frame}{ AdaBoost: Freund \& Schapire, 1996}
\begin{enumerate}
\item Weight observations equally $w_i = \frac{1}{N}$. for all $i=1,\ldots,N$.
\item From $b=1,\ldots, B$ do the following:
\begin{enumerate}
\item Fit a model  $f^{(b)}(x_i)$ to training data using $w_i$.
\item Compute $err_b = \frac{\sum_{i=1}^N w_i I[ y_i \neq f^{(b)}(x_i) ]}{\sum_{i=1}^N w_i}$
\item Compute $\alpha_b = \log \left(  \frac{1-err_b}{err_b}\right)$
\item Update weights for $i=1,\ldots,N$: 
\begin{align*}
w_i \leftarrow w_i \cdot \exp[\alpha_b I (y_i \neq f^{(b)}(x_i) ]
\end{align*}
renormalize so that $\sum w_i =1$.
\end{enumerate}
\item Compute $\hat{f}(x) = sign[ \sum_{b} \alpha_b \hat{f}^{(b)}(x_i)]$.
\end{enumerate}
\end{frame}


\begin{frame}{Additive Models}
\begin{itemize}
\item $\hat{f}(x) = \sum_{b} \alpha_b \hat{f}^{(b)}(x_i)]$ is an \alert{additive model}
\item Lots of examples in the literature:
\begin{itemize}
\item  Basis Functions (ie: polynomials) $\sum_{k=1}^K \theta_k g_k(x)$.
\item GAMs: $f(x) = \sum_{k=1}^K f_k(x)$.
\end{itemize}
\item Usually we fit these in \alert{one step} (OLS, MLE, etc.)
\item Here we fit $\alpha_b, \theta_b$ in a \alert{stagewise} manner.
\item Process fits slowly but avoids overfitting problem.
\end{itemize}
\end{frame}




\begin{frame}{What about Gradient Boosting?}
Same as AdaBoost with some modifications
\begin{itemize}
\item Loss function $L(y_i,\gamma)$
\item Choose $\gamma$ to minimize $\min L(y_i,\gamma)$ and get $h_b(x)$.
\item Update $r_i = \left[ \frac{\partial L(y_i, F(x_i))}{ \partial F(x_i)} \right]$ this is the \alert{gradient} evaluated at $b$th iteration of $F_b(x_i)$.
\item Update $F_b(x) = F_{b-1}(x) + \gamma_b h_b(x)$.
\end{itemize}
\end{frame}


\begin{frame}{How do we do this in R?}
\begin{itemize}
\item Sample Splitting, Bagging, etc. \texttt{caret}, \texttt{sample}
\item Gradient Boosting. \texttt{gbm} (easy), \texttt{xgboost} (faster).
\item Random forest \texttt{randomForest}.
\end{itemize}
\end{frame}



\end{document}
