---
title: "Problem Set 2"
author: "Prasanna Parasurama"
date: "Due: 3/1/19"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newcommand{\E}[1]{\ensuremath{\mathbb{E}\big[#1\big]}}
\newcommand{\Emeasure}[2]{\ensuremath{\mathbb{E}_{#1}\big[#2\big]}}

## Problem 1 (Analytical Exercise)

Consider the estimation of the individual effects model:

\begin{align*}
	y_{it} = x_{it}'\beta + \alpha_{i} + \epsilon_{it} \mbox{,     } \E{\epsilon \vert x_{it},\alpha_{i}} = 0
\end{align*}
where $i=\{1,...,n\}$ and $t=\{1,...,T\}$.
\newline
This exercises ask you to relate the (random effects) GLS estimator $\hat{\beta}_{GLS}=(X_{*}'X_{*})^{-1}X_{*}'y_{*}$ to the "within" (fixed-effects) estimator $\hat{\beta}_{FE}=(\dot{X}'\dot{X})\dot{X}'\dot{y}$ and the "between" estimator $\hat{\beta}_{BW}=(\bar{X}'\bar{X})^{-1}\bar{X}'\bar{y}$ where $w=\{x,y\}$:
\begin{align*}
	\bar{w}_{i} &:= \frac{1}{T} \sum_{i=1}^{T} w_{i} \\
	\dot{w}_{i} &:= w_{it} - \bar{w}_{i} \\
	w_{it,*} &:= w_{it} - (1-\lambda)\bar{w}_{i} \\
	\lambda^{2} &= \frac{Var(\epsilon)}{T\,Var(\alpha_{i}) + Var(\epsilon_{it})}
\end{align*}

\begin{enumerate}
	\item Express the GLS estimator in terms of $\bar{X}$, $\dot{X}$, $\bar{y}$, $\dot{y}$, $\lambda$, and $T$.
	
	Let $M$ be the transformation matrix such that $MX = \dot{X}$ \\
	Let $P$ be the transformation matrix such that $PX = \bar{X}$
	
\begin{align}
  \hat{\beta}_{GLS} &= [X'(M+\lambda P)X]^{-1}[X'(M+\lambda P)y]
\end{align}
	
Consider $X'(M+\lambda P)X$. Note that $M$ and $P$ are idempotent. That is, $M^2=M$ and $P^2=P$.

\begin{align}
  X'(M+\lambda P)X &= X'(MM+\lambda PP)X \\
  &= X'(MMX + \lambda PPX) \\
  &= X'(M\dot{X} + \lambda P\bar{X}) \\
  &= (\dot{X}'\dot{X} + \lambda \bar{X}' \bar{X})
\end{align}

Likewise: 
\begin{align}
X'(M+\lambda P)y = (\dot{X}'\dot{y} + \lambda \bar{X}' \bar{y})
\end{align}

Plugging these back into (1):
\begin{align}
  \hat{\beta}_{GLS} &= [(\dot{X}'\dot{X} + \lambda \bar{X}' \bar{X})]^{-1}(\dot{X}'\dot{y} + \lambda \bar{X}' \bar{y})
\end{align}

	\item Show that there is a matrix R depending on $\bar{X}$, $\dot{X}$, $\lambda$ and $T$ such that the GLS estimator is a weighted average of the "within" and "between" estimators: 
	\begin{align*}
		\hat{\beta}_{GLS} &= R \hat{\beta}_{FE} + (I-R)\hat{\beta}_{BW} \\
		&= R \hat{\beta}_{FE} + \hat{\beta}_{BW}-R\hat{\beta}_{W}
	\end{align*}
where, 
\begin{align*}
	R = [\dot{X}'\dot{X} + \lambda \bar{X}' \bar{X}]^{-1}\dot{X}'\dot{X} \\
	\hat{\beta}_{FE}=(\dot{X}'\dot{X})^{-1}\dot{X}'\dot{y} \\
	\hat{\beta}_{BW}=(\bar{X}'\bar{X})^{-1}\bar{X}'\bar{y} \\
\end{align*}

	\item What happens to the relative weights on the "within" and "between" estimators as we increase the sample size, i.e. $T \to \infty$?
	
As $T \to \infty$, $\lambda \to 0$. This implies:

$$\hat{\beta}_{GLS} \to [\dot{X}'\dot{X}]^{-1}(\dot{X}'\dot{y}) = \hat{\beta}_{FE}$$

So, so full weight is given to fixed effects estimator, and $\hat{\beta}_{GLS}$ behaves like $\hat{\beta}_{FE}$


	\item Suppose that the random effects assumption $\E{\alpha_{i} \vert x_{i1,...,x_{iT}}} = 0$ does not hold. Characterize the bias of the estimators $\hat{\beta}_{FE}$, $\hat{\beta}_{W}$. (Note: An estimator $\hat{\beta}$ is unbiased if $\E{\hat{\beta}}=\beta$)


$$\E{\hat{\beta}_{FE}|X} = \E{(\dot{X}'\dot{X})^{-1}\dot{X}'\dot{y}|X}$$
where, $\dot{y} = \dot{X}\beta+\epsilon$. 

\begin{align*}
\E{\hat{\beta}_{FE}|X} &= \E{(\dot{X}'\dot{X})^{-1}\dot{X}'(\dot{X}\beta+\epsilon)|X} \\
&= \E{(\dot{X}'\dot{X})^{-1}\dot{X}'\dot{X}\beta|X} + 
\E{(\dot{X}'\dot{X})^{-1}\dot{X}'\epsilon)|X} \\
&= \beta + (\dot{X}'\dot{X})^{-1}\dot{X}'\E{\epsilon|X} \\
&= \beta
\end{align*}
since $\E{\epsilon|X} = 0$. Therefore, fixed effects estimator is unbiased. 

$$\E{\hat{\beta}_{BW}|X} = \E{(\bar{X}'\bar{X})^{-1}\bar{X}'\bar{y}|X}$$
where, $\bar{y} = \bar{X}\beta+\alpha+\epsilon$. 
\begin{align*}
\E{\hat{\beta}_{BW}|X} &= \E{(\bar{X}'\bar{X})^{-1}\bar{X}'(\bar{X}\beta+\epsilon+\alpha)|X} \\
&= \E{(\bar{X}'\bar{X})^{-1}\bar{X}'\bar{X}\beta|X} + 
\E{(\bar{X}'\bar{X})^{-1}\bar{X}'\epsilon+\alpha)|X} \\
&= \beta + (\bar{X}'\bar{X})^{-1}\bar{X}'\E{\epsilon+\alpha|X} \\
&= \beta + (\bar{X}'\bar{X})^{-1}\bar{X}'\E{\alpha|X}
\end{align*}

Note that the between estimator will be biased, since $\E{\alpha|X} \ne 0$.

	\item Use your result from $(d)$ to give a formula for the bias of our random effects estimator $\hat{\beta}_{GLS}$. What happens to the bias as $T \to \infty$.
	
$$\hat{\beta}_{GLS} = R \hat{\beta}_{FE} + (I-R)\hat{\beta}_{BW}$$
\begin{align*}
	\E{\hat{\beta}_{GLS}|X} &= R \E{\hat{\beta}_{FE}|X} + (I-R)E{\hat{\beta}_{BW}|X} \\
	&= R \beta + (I-R) (\beta + (\bar{X}'\bar{X})^{-1}\bar{X}'\E{\alpha|X})
\end{align*}

Since $\hat{\beta}_{GLS} \to \hat{\beta}_{FE}$ as $T \to \infty$, bias will tend to zero (asymptotically consistent).

\end{enumerate}

## Problem 2 (Coding Exercise)

We observe $N$ observations of the random variable $X_{i}$ where each $X_{i}$ is drawn from the Weibull distribution:

\begin{align*}
	X_{i} \sim W(\gamma)
\end{align*}

The probability density function for the Weibull is the following:

\begin{align*}
	f(x;\gamma) = \gamma x^{\gamma - 1} \exp(-(x^{\gamma})) \;\; ; x \geq 0 , \gamma > 0
\end{align*}

1. Assume our $N$ observations are independent and identically distributed, what is the log-likelihood function?

The Likelihood function is:
	\begin{align*}
L(\gamma|x) &= \prod_{i=1}^{N} f(x_i)  \\
			&= \prod_{i=1}^{N} \gamma x^{\gamma - 1} \exp(-x^{\gamma}) \\
			&= \gamma^N \exp(-\sum_{i=1}^{N}x^\gamma)\prod_{i=1}^{N}x^{\gamma-1}
\end{align*}

Taking the log, we get the log-likelihood function:

\begin{align*}
l(\gamma|x) &= N ln(\gamma)-\sum_{i=1}^{N}x^\gamma + (\gamma-1) \sum_{i=1}^{N}ln(x)\\
\end{align*}

2. Calculate the gradient (or first derivative) of your log-likelihood function.
	
\begin{align*}
\frac{\partial l}{\partial \gamma} = \frac{N}{\gamma}-\sum_{i=1}^{N} ln(x_i)x_i^\gamma + \sum_{i=1}^{N}ln(x_i)
\end{align*}

3. Using the first order condition, what is the MLE estimator for $\gamma$?
	
\begin{align*}
\frac{\partial l}{\partial \gamma} = \frac{N}{\gamma}-\sum_{i=1}^{N} ln(x_i)x_i^\gamma + \sum_{i=1}^{N}ln(x_i) = 0
\end{align*}

There doesn't seem to be a closed form solution for $\gamma$

4. Verify that the second order condition guarantees a unique global solution. 

\begin{align*}
\frac{\partial^2 l}{\partial \gamma^2} = -\frac{N}{\gamma^2}-\sum_{i=1}^{N} (ln(x_i))^2 x_i^\gamma < 0
\end{align*}

The second derivative is always negative, which guarentees an unique global maximum. 

5. In R, I want you to write a function called mle\_weibull that takes two arguments $(X,\gamma)$, where $X$ is a vector of data and $\gamma$ is a scalar. The function returns the value of the log-likelihood function you derived in the last part.

```{r}
mle_weibull = function(X, g){
  N = length(X)
  l = N*log(g) - sum(X^g) + (g-1)*(sum(log(X)))
  
  # return negative, since optim function minimizes instead of maximizes.
  return(-l)
  
}
```

6. Optimiziation routines can either be given a first derivative (or gradient) or the optimization routines calculate numerical derivatives. We will be using the R function $optim$, which accepts the first derivative as an argument $gr$. 

  a. We first want you to run $optim$ without supplying a first derivative (leaving gr out of the function). Note, to run optim you will need to supply your data $X$ as an additional parameter at the end of the function. We have provided you with simulated data in the file 'prob\_4\_simulation.rda' located in the data folder. 
```{r}
load('data/prob_4_simulation.rda')
optim(par=1, fn=mle_weibull, X=sim)
```
		
		
  b. We now want you to create a new function called gradient, which takes the same two arguments as your likelihood function. Now calculate the MLE using optim with the gradient.
```{r}
gradient = function(X, g){
  N = length(X)
  d = N/g - sum(log(X)*(X^g)) + sum(log(X))
  return(d)
}
```

```{r}
optim(par=1, fn=mle_weibull, gr=gradient, X=sim)
```
 
  c. Compare both the number of iterations until convergence and your estimated $\gamma$ values from both runs.
		
$optim$ doesn't seem to be using the gradient function. The number of iterations are same in both cases (30). 

However, the optimal value $\hat{\gamma_{MLE}} = 1.982227$ seems to be correct: 

```{r}
print(gradient(sim, 1.5))
print(gradient(sim, 1.982227))
print(gradient(sim, 2.5))
```







